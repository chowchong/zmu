"""
Subgen - æ™ºèƒ½æ¨¡åž‹ç®¡ç†ç³»ç»Ÿ
æ”¯æŒè‡ªåŠ¨ä¸‹è½½ã€ç‰ˆæœ¬æ£€æŸ¥ã€ç¦»çº¿ä½¿ç”¨
"""

import os
import json
from pathlib import Path
from typing import Dict, Optional, List
import shutil


class ModelManager:
    """æ¨¡åž‹ç®¡ç†å™¨"""
    
    # æ¨¡åž‹é…ç½®
    MODELS = {
        'funasr': {
            'name': 'FunASR Paraformer (å®žéªŒæ€§)',
            'model_id': 'iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch',
            'required': False,  # Changed from True
            'size_mb': 220,
            'languages': ['zh', 'zh-CN'],
            'best_for': 'âš ï¸ å®žéªŒæ€§åŠŸèƒ½ - VADè¿‡äºŽä¸¥æ ¼å¯¼è‡´è¯†åˆ«ä¸å®Œæ•´',
            'provider': 'modelscope'
        },
        'sensevoice': {
            'name': 'SenseVoice Small (å®žéªŒæ€§)',
            'model_id': 'iic/SenseVoiceSmall',
            'required': False,
            'size_mb': 80,
            'languages': ['zh', 'en', 'ja', 'ko', 'yue'],
            'best_for': 'âš ï¸ å®žéªŒæ€§åŠŸèƒ½ - ä¸æ”¯æŒæ—¶é—´æˆ³ï¼Œæ— æ³•ç”Ÿæˆå­—å¹•',
            'provider': 'modelscope'
        },
        'whisper_small': {
            'name': 'Whisper Small (å¤šè¯­è¨€)',
            'model_id': 'small',
            'required': True,
            'size_mb': 244,
            'languages': ['en', 'multi'],
            'best_for': 'è‹±æ–‡å†…å®¹ã€å¤šè¯­è¨€æ··åˆ',
            'provider': 'openai'
        },
        'whisper_medium': {
            'name': 'Whisper Medium (é«˜è´¨é‡)',
            'model_id': 'medium',
            'required': True,
            'size_mb': 769,
            'languages': ['en', 'multi'],
            'best_for': 'é«˜è´¨é‡è‹±æ–‡è¯†åˆ«',
            'provider': 'openai'
        },
        'whisper_large': {
            'name': 'Whisper Large-v3 (æœ€å¼º)',
            'model_id': 'large-v3',
            'required': False,
            'size_mb': 1550,
            'languages': ['all'],
            'best_for': 'å¤æ‚å£éŸ³ã€ä¸“ä¸šæœ¯è¯­',
            'provider': 'openai'
        }
    }
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡åž‹ç®¡ç†å™¨"""
        import sys
        
        # Check if running in frozen mode (App Bundle)
        if getattr(sys, 'frozen', False):
            # Running as compiled app
            # sys.executable is inside MacOS/ likely, so we go up to Contents/Resources
            # Example: App.app/Contents/MacOS/App -> App.app/Contents/Resources
            base_path = Path(sys.executable).parent.parent / 'Resources'
            self.models_dir = base_path / 'models'
            self.cache_dir = base_path / 'cache' # Local cache inside app
        else:
            # Running from source
            self.models_dir = Path.home() / '.subgen' / 'models'
            self.cache_dir = Path.home() / '.cache'
            
        self.models_dir.mkdir(parents=True, exist_ok=True)
        # We might need to handle cache dir creation carefully if in read-only bundle, 
        # but user wants to manage it, so we assume write access or user action.
        
        self.config_file = self.models_dir / 'config.json'
        # Ensure config file exists or copy from resources if strictly needed, 
        # but for now we follow existing logic.
        self.config = self._load_config()
    
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®"""
        if self.config_file.exists():
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {'installed_models': {}}
    
    def _save_config(self):
        """ä¿å­˜é…ç½®"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
    
    def check_installation(self) -> Dict[str, bool]:
        """æ£€æŸ¥æ‰€æœ‰æ¨¡åž‹å®‰è£…çŠ¶æ€"""
        status = {}
        for key, info in self.MODELS.items():
            if info['provider'] == 'openai':
                status[key] = self._check_whisper_model(info['model_id'])
            elif info['provider'] == 'modelscope':
                status[key] = self._check_funasr_model(info['model_id'])
        return status
    
    def _check_whisper_model(self, model_id: str) -> bool:
        """æ£€æŸ¥ Whisper æ¨¡åž‹"""
        try:
            model_path = self.cache_dir / 'whisper' / f"{model_id}.pt"
            return model_path.exists()
        except:
            return False
    
    def _check_funasr_model(self, model_id: str) -> bool:
        """æ£€æŸ¥ FunASR æ¨¡åž‹"""
        try:
            # Check default path (modelscope/model_id)
            base_cache = self.cache_dir / 'modelscope'
            path1 = base_cache / model_id
            if path1.exists():
                return True
                
            # Check hub path (modelscope/hub/model_id_with_underscore) - legacy/alternative
            path2 = base_cache / 'hub' / model_id.replace('/', '_')
            if path2.exists():
                return True
                
            return False
        except:
            return False

    def _download_whisper_model(self, model_id: str):
        """ä¸‹è½½ Whisper æ¨¡åž‹"""
        print(f"    â³ ä¸‹è½½ Whisper {model_id} æ¨¡åž‹...")
        try:
            import whisper
            # Redirect download to our cache dir
            # Whisper download_root defaults to ~/.cache/whisper
            download_root = str(self.cache_dir / 'whisper')
            Path(download_root).mkdir(parents=True, exist_ok=True)
            
            # We can use whisper._download directly or set env var?
            # Easiest is to rely on load_model's download_root param if available,
            # or pre-download using torch.hub.download_url_to_file logic manually?
            # Actually whisper.load_model accepts 'download_root'.
            
            model = whisper.load_model(model_id, download_root=download_root)
            del model  # é‡Šæ”¾å†…å­˜
            print(f"    ðŸ“¥ æ¨¡åž‹å·²ç¼“å­˜")
        except Exception as e:
            raise Exception(f"Whisper æ¨¡åž‹ä¸‹è½½å¤±è´¥: {e}")
    
    def _download_funasr_model(self, model_id: str):
        """ä¸‹è½½ FunASR æ¨¡åž‹"""
        print(f"    â³ ä¸‹è½½ FunASR æ¨¡åž‹...")
        try:
            from modelscope.hub.snapshot_download import snapshot_download
            model_dir = snapshot_download(
                model_id,
                cache_dir=str(self.cache_dir / 'modelscope')
            )
            print(f"    ðŸ“¥ æ¨¡åž‹å·²ç¼“å­˜")
        except Exception as e:
            raise Exception(f"FunASR æ¨¡åž‹ä¸‹è½½å¤±è´¥: {e}")
    
    def get_recommended_model(self, language: Optional[str] = None) -> str:
        """æ™ºèƒ½æŽ¨èæ¨¡åž‹"""
        if language and language.startswith('zh'):
            return 'funasr'
        return 'whisper_medium'
    
    def list_installed_models(self) -> List[Dict]:
        """åˆ—å‡ºå·²å®‰è£…çš„æ¨¡åž‹"""
        status = self.check_installation()
        installed = []
        for key, is_installed in status.items():
            if is_installed:
                info = self.MODELS[key].copy()
                info['key'] = key
                installed.append(info)
        return installed
    
    def install_required_models(self):
        """å®‰è£…æ‰€æœ‰å¿…éœ€æ¨¡åž‹"""
        print("\n" + "="*60)
        print("ðŸš€ Subgen é¦–æ¬¡è¿è¡Œï¼šæ­£åœ¨å®‰è£…å¿…éœ€æ¨¡åž‹")
        print("="*60 + "\n")
        
        status = self.check_installation()
        required_models = [k for k, v in self.MODELS.items() if v['required']]
        to_install = [k for k in required_models if not status.get(k, False)]
        
        if not to_install:
            print("âœ… æ‰€æœ‰å¿…éœ€æ¨¡åž‹å·²å®‰è£…\n")
            return
        
        total_size = sum(self.MODELS[k]['size_mb'] for k in to_install)
        print(f"ðŸ“¦ éœ€è¦ä¸‹è½½ {len(to_install)} ä¸ªæ¨¡åž‹ï¼Œæ€»å¤§å°çº¦ {total_size} MB\n")
        
        for idx, model_key in enumerate(to_install, 1):
            model_info = self.MODELS[model_key]
            print(f"[{idx}/{len(to_install)}] {model_info['name']}")
            print(f"    å¤§å°: {model_info['size_mb']} MB")
            print(f"    ç”¨é€”: {model_info['best_for']}\n")
            
            try:
                self.download_model(model_key)
                print(f"    âœ… å®‰è£…æˆåŠŸ\n")
            except Exception as e:
                print(f"    âŒ å®‰è£…å¤±è´¥: {e}\n")
                raise
        
        print("="*60)
        print("ðŸŽ‰ æ‰€æœ‰å¿…éœ€æ¨¡åž‹å®‰è£…å®Œæˆï¼")
        print("="*60 + "\n")
    
    def download_model(self, model_key: str):
        """ä¸‹è½½æŒ‡å®šæ¨¡åž‹"""
        if model_key not in self.MODELS:
            raise ValueError(f"æœªçŸ¥æ¨¡åž‹: {model_key}")
        
        model_info = self.MODELS[model_key]
        
        if model_info['provider'] == 'openai':
            self._download_whisper_model(model_info['model_id'])
        elif model_info['provider'] == 'modelscope':
            self._download_funasr_model(model_info['model_id'])
        
        self.config['installed_models'][model_key] = {'version': '1.0'}
        self._save_config()
    
    
    def _download_funasr_model(self, model_id: str):
        """ä¸‹è½½ FunASR æ¨¡åž‹"""
        print(f"    â³ ä¸‹è½½ FunASR æ¨¡åž‹...")
        try:
            from modelscope.hub.snapshot_download import snapshot_download
            model_dir = snapshot_download(
                model_id,
                cache_dir=str(Path.home() / '.cache' / 'modelscope')
            )
            print(f"    ðŸ“¥ æ¨¡åž‹å·²ç¼“å­˜")
        except Exception as e:
            raise Exception(f"FunASR æ¨¡åž‹ä¸‹è½½å¤±è´¥: {e}")
    
    def get_recommended_model(self, language: Optional[str] = None) -> str:
        """æ™ºèƒ½æŽ¨èæ¨¡åž‹"""
        if language and language.startswith('zh'):
            return 'funasr'
        return 'whisper_medium'
    
    def list_installed_models(self) -> List[Dict]:
        """åˆ—å‡ºå·²å®‰è£…çš„æ¨¡åž‹"""
        status = self.check_installation()
        installed = []
        for key, is_installed in status.items():
            if is_installed:
                info = self.MODELS[key].copy()
                info['key'] = key
                installed.append(info)
        return installed


def main():
    """å‘½ä»¤è¡Œå·¥å…·"""
    import sys
    
    manager = ModelManager()
    
    if len(sys.argv) < 2:
        print("ðŸ“¦ Subgen - æ¨¡åž‹ç®¡ç†å·¥å…·\n")
        print("ç”¨æ³•:")
        print("  python model_manager.py check    # æ£€æŸ¥æ¨¡åž‹çŠ¶æ€")
        print("  python model_manager.py install  # å®‰è£…å¿…éœ€æ¨¡åž‹")
        return
    
    command = sys.argv[1]
    
    if command == 'check':
        print("ðŸ” æ£€æŸ¥æ¨¡åž‹å®‰è£…çŠ¶æ€...\n")
        status = manager.check_installation()
        for key, info in manager.MODELS.items():
            icon = "âœ…" if status.get(key, False) else "âŒ"
            print(f"  {icon} {info['name']} ({key})")
    
    elif command == 'install':
        manager.install_required_models()
    
    else:
        print(f"æœªçŸ¥å‘½ä»¤: {command}")

if __name__ == "__main__":
    main()
    def _download_model_with_progress(self, model_key: str, progress_callback=None):
        """ä¸‹è½½æ¨¡åž‹å¹¶æŠ¥å‘Šè¿›åº¦ï¼ˆç”¨äºŽåŽå°ä¸‹è½½ï¼‰"""
        if model_key not in self.MODELS:
            return False
        
        model_info = self.MODELS[model_key]
        model_type = model_info['type']
        
        try:
            if model_type == 'whisper':
                # For Whisper, we'll use the existing download method
                # Report progress at milestones since whisper doesn't provide granular progress
                if progress_callback:
                    progress_callback(0, model_info['size_mb'] * 1024 * 1024)
                
                self._download_whisper_model(model_info['id'])
                
                if progress_callback:
                    progress_callback(model_info['size_mb'] * 1024 * 1024, model_info['size_mb'] * 1024 * 1024)
                
                return True
                
            elif model_type == 'funasr':
                if progress_callback:
                    progress_callback(0, model_info['size_mb'] * 1024 * 1024)
                
                self._download_funasr_model(model_info['id'])
                
                if progress_callback:
                    progress_callback(model_info['size_mb'] * 1024 * 1024, model_info['size_mb'] * 1024 * 1024)
                
                return True
            
            return False
            
        except Exception as e:
            print(f"ä¸‹è½½å¤±è´¥: {e}")
            return False
